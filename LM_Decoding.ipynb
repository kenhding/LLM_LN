{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Search**\n",
        "\n",
        "It suits occasion require fast inference but less quality.\n",
        "- Pros: fast\n",
        "- Cons: if a wrong word selected, it influences the subsequent prediction. The text generated is plain, easily fall into local minimum. (can use penalty)\n"
      ],
      "metadata": {
        "id": "CczzNZ_VMQQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "print(input_ids.shape)\n",
        "# this contains the token index for token vector (14 tokens including\n",
        "# punctuation) however further embedding will not use way like one-hot\n",
        "# vectors but embedding vector 768"
      ],
      "metadata": {
        "id": "Sy9sI_K1Et1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff6c664-3a03-4888-bfcd-349fe7cb1f63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(logits):\n",
        "    return torch.argmax(logits, dim=-1).unsqueeze(-1)"
      ],
      "metadata": {
        "id": "HUF6OK8J-NdE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lm_decoding(model, max_new_tokens, seq_ids, eos_token_id, decode_func):\n",
        "    # the step is to (1) take the model output, (2) use argmax to get the tocken id,\n",
        "    #  (3) concatenate the id to the end of the sequence\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(seq_ids)\n",
        "            next_token_logits = outputs.logits[:, -1, :] # the dimension is\n",
        "            # [batch size, token length, vocab size], so we take the last token,\n",
        "            # this logis is the value before softmax\n",
        "            # The argmax of the logits is equivalent to the argmax of the\n",
        "            # softmax probabilities.\n",
        "            next_token_id = decode_func(next_token_logits)\n",
        "\n",
        "            if next_token_id == eos_token_id:\n",
        "                print(\"The output sequence comes to an end.\")\n",
        "                break\n",
        "            seq_ids = torch.cat([seq_ids, next_token_id], dim=-1)\n",
        "\n",
        "    return seq_ids"
      ],
      "metadata": {
        "id": "FS7ndj8oEYRl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "decode_ids = lm_decoding(model, max_new_tokens, input_ids.clone(), tokenizer.eos_token_id, greedy_search)\n",
        "generated_text = tokenizer.decode(decode_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVBVdGxkGU0E",
        "outputId": "5aaf817b-0278-4a9b-fea4-5ce7ead9c8fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain,  the most difficult path to the top of the mountain.  The most difficult path to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam Search**\n",
        "\n",
        "It keeps num of beams (width) choices (when beam = 1, it is identical as greedy search). It is heristic method. The final output is the one with highest cumulative probability. This method tackled the limitation of greedy search.\n",
        "\n",
        "- Cons: the optimal sequence may not be selected and requires large space and computational resource.\n",
        "\n",
        "- Varitions:\n",
        "    * Speculative Beam Search for Simultaneous Translation https://doi.org/10.48550/arXiv.1909.05421\n",
        "    * Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models https://doi.org/10.48550/arXiv.1610.02424\n",
        "    * Cluster-based beam search for pointer-generator chatbot grounded by knowledge https://doi.org/10.1016/j.csl.2020.101094\n",
        "    * Beam-search SIEVE for low-memory speech recognition https://doi.org/10.21437/Interspeech.2024-2457\n",
        "\n",
        "\n",
        "\n",
        "`Reference:`\n",
        "\n",
        "- https://www.width.ai/post/what-is-beam-search\n",
        "- https://doi.org/10.48550/arXiv.1805.04833"
      ],
      "metadata": {
        "id": "gB5GDMNI0G46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(prompt, model, tokenizer, max_new_tokens=20, beam_size=3, alpha=0.7):\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    current_ids = input_ids.clone()\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    current_beam = [(current_ids, 0.0)] # init the current idx and probability\n",
        "    finished_sequence = []\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        beam_pool = [] #tmp list for beam\n",
        "\n",
        "        # except the initial iteration, the current beam has 3 sequence\n",
        "        for i_sequence, i_log_prob in current_beam:\n",
        "\n",
        "            if i_sequence[0,-1] == eos_token_id:\n",
        "                finished_sequence.append((i_sequence, i_log_prob))\n",
        "                continue\n",
        "            # for each beam it gets the top beam_size next token\n",
        "            with torch.no_grad():\n",
        "                outputs = model(i_sequence)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "                log_probs = torch.log_softmax(next_token_logits.ravel(), dim=-1)\n",
        "                top_k_log_probs, top_k_ids = torch.topk(log_probs, beam_size)\n",
        "\n",
        "            # loop over each new token\n",
        "            for i in range(beam_size):\n",
        "                token_id = top_k_ids[i]\n",
        "                log_prob = top_k_log_probs[i]\n",
        "\n",
        "                # cat the previous sequence plus one beam token\n",
        "                new_sequence = torch.cat([i_sequence, token_id.reshape(1,-1)], dim=1)\n",
        "                # cumulate the probability\n",
        "                beam_pool.append((new_sequence, log_prob + i_log_prob))\n",
        "\n",
        "        beam_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "        current_beam = beam_pool[:beam_size]# select the top 3 to the current beam\n",
        "\n",
        "    finished_sequence.extend(current_beam)\n",
        "\n",
        "    # Since probability is between 0 to 1 while log < 1 is alwasy\n",
        "    # negative numbers, so cumulative probability of a long sequence will have\n",
        "    # smaller prabability. The solution is to normalize\n",
        "    normalized_score_key = lambda x: x[1] / (x[0].size(1)**alpha) # by alpha\n",
        "    best_sequence = max(finished_sequence, key=normalized_score_key)\n",
        "\n",
        "    generated_text = tokenizer.decode(best_sequence[0].squeeze(), skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "f8wxYkbeKX4R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example 1\n",
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "print(f\"Example 1: greedy search with beam size of 1 \\n {beam_search(prompt, model, tokenizer, max_new_tokens=20, beam_size=1, alpha=0.7)}\\n\")\n",
        "\n",
        "# example 2\n",
        "prompt = \"The earth is the place\"\n",
        "print(f\"Example 2: beam search with size of 5\\n {beam_search(prompt, model, tokenizer, max_new_tokens=10, beam_size=5, alpha=0.5)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blTbDBY--Bd7",
        "outputId": "d624c1d6-b242-4452-93b1-87e380a1a1b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: greedy search with beam size of 1 \n",
            " There is more than one path to the top of the mountain,  the most difficult path to the top of the mountain.  The most difficult path to the\n",
            "\n",
            "Example 2: beam search with size of 5\n",
            " The earth is the place where the sun shines.”\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top K Sampling**"
      ],
      "metadata": {
        "id": "AtygrdOMeU2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "ej_uDAqMExgR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits, top_k):\n",
        "    # select top k candidates and set the threshold\n",
        "    top_k_log_probs, _ = torch.topk(logits, top_k)\n",
        "    min_k_prob = top_k_log_probs[:, -1]\n",
        "    # create a new logit by setting -inf to all probs below min_k_prob\n",
        "    new_logits = torch.where(next_token_logits < min_k_prob, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "    # renormalize the probs\n",
        "    normalized_probs = torch.softmax(new_logits, dim=-1)\n",
        "    return torch.multinomial(normalized_probs, num_samples=1)\n"
      ],
      "metadata": {
        "id": "HIDZwKfY_bvJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "eos_token_id = tokenizer.eos_token_id # end of sequence\n",
        "current_ids = input_ids.clone() # deep copy\n",
        "top_k = 10\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        next_token_id = top_k_sampling(next_token_logits, top_k)\n",
        "\n",
        "        if next_token_id == eos_token_id:\n",
        "            print(\"The output sequence comes to an end.\")\n",
        "            break\n",
        "\n",
        "        current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "QNIkJ0oweWto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c10c257-0cdf-4ae9-9842-13afa73bc46b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain,  a path to where one is not only able to find the right route to where one needs to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top P Sampling**\n",
        "\n",
        "Reference: https://arxiv.org/pdf/1904.09751"
      ],
      "metadata": {
        "id": "sfJ4M3KdFzrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "2H2ghuYDFyJT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(logits, top_p):\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    # sort and cumulate probs\n",
        "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    # thresholding\n",
        "    sorted_idx_remove = cum_probs > top_p\n",
        "    sorted_idx_remove[..., 1:] = sorted_idx_remove[..., :-1].clone()\n",
        "    sorted_idx_remove[..., 0] = False\n",
        "    # fill -inf to idx above threshold\n",
        "    index_remove = sorted_idx[sorted_idx_remove]\n",
        "    logits.scatter_(dim=-1, index=index_remove.unsqueeze(0), value=float(\"-inf\"))\n",
        "    # normalize and sample\n",
        "    normalized_probs = torch.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(normalized_probs, num_samples=1)"
      ],
      "metadata": {
        "id": "TBc7AjHx_9iY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "eos_token_id = tokenizer.eos_token_id # end of sequence\n",
        "current_ids = input_ids.clone() # deep copy\n",
        "top_p = 0.8\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        next_token_id = top_p_sampling(next_token_logits, top_p)\n",
        "\n",
        "        if next_token_id == eos_token_id:\n",
        "            print(\"The output sequence comes to an end.\")\n",
        "            break\n",
        "\n",
        "        current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "niKsP-9f92Vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47fcc9e9-71b9-4ae6-98fe-da66b5011422"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain, vernacular in the moment,‏‏‏‏‏‏‏\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Termperture**\n"
      ],
      "metadata": {
        "id": "qCi30C8UU6SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "1RsBnrUNN_IC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "eos_token_id = tokenizer.eos_token_id # end of sequence\n",
        "current_ids = input_ids.clone() # deep copy\n",
        "top_p = 0.8\n",
        "temperture_val = 0.7\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        # temperture + top k sampling\n",
        "        next_token_logits = next_token_logits / temperture_val\n",
        "        next_token_id = top_p_sampling(next_token_logits, top_p)\n",
        "\n",
        "        if next_token_id == eos_token_id:\n",
        "            print(\"The output sequence comes to an end.\")\n",
        "            break\n",
        "\n",
        "        current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktjmi3VQWExM",
        "outputId": "b6775698-b826-403d-e41a-8f13229d790a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain,   the one that leads to the very top of the mountain.  The one that leads to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eriCvI0QpFPv"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}
