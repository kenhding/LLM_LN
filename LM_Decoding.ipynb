{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Hao Ding\n",
        "\n",
        "Data: 15.Dec.2025\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y3xhagoQoop-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The note book contains following:\n",
        "\n",
        "- Greedy search\n",
        "- Beam search\n",
        "- Top-K sampling\n",
        "- Top-P sampling\n",
        "- Temperture"
      ],
      "metadata": {
        "id": "P0e4y6R3h-e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example for decoding algorithms**"
      ],
      "metadata": {
        "id": "6S_y4d9RDij7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esi86EIaBm17",
        "outputId": "a667fa6a-db80-44c4-df0c-370fc5ef5ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# load small model and tokenizer\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "print(model)\n",
        "# see model architecture below, it has embedding layer size of\n",
        "# 50257 (vocab size), 768 (embedding dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "f47115ffc2f34374845ffc87d54b13b3",
            "7055174513794b8a92bf00753cdc5249",
            "3a5073070fb244c189adbbf15975eaca",
            "67302b4b862c4d74a0eb43a291a744d9",
            "0da808adbd874249b7e7f2cb95fc2a5f",
            "5c9e14dbb476405291cbfef3330f0641",
            "5f8007f537cc450e8c54ec2bdaa88250",
            "32ae7b3131ac40a18679b323226a6ab5",
            "25c05fac953246a48b1e6c1b2b460577",
            "0a8185ae18a94f8b82620d54091aa53b",
            "a0f7a74628b44065b545e6037359f6f1",
            "da96a0fbb5dd45b6be79a693088d9ef2",
            "afc62f81aaf645a28b332e7cf696c97b",
            "71e9bae9c0f248e685b3e77b1d7198b0",
            "48aad6dc02fa49168d8cf8aa21ad3897",
            "64767d6106954eb2b0b9cc59391fa50c",
            "be5ce48ec3384c369dc39f5d905737d0",
            "7c96a53d72424591b33f648bebb8341f",
            "4cda1b2c1920438ab03aaa4410d889fd",
            "9b774a7b4cfe4cb5af1a2aab54a4159f",
            "0c45801a68d648af9d4513a9b4742c84",
            "09d68e9031e148d8bab618f2cd72ea9b",
            "74c05cad30614bd3b05d4da8a5838b57",
            "edbf7c5563e748e78eb0871181c5d39f",
            "2deea5d90cdd4bd790ec2af00cbc62e9",
            "a460f2d8240e48559444bad1d851385c",
            "1cf421396a704d978facd080a96eedba",
            "355a48a64d084d64b133a92bb067d745",
            "2235dde856cd4f748f88fe7ef5b3c883",
            "3d5886558b474676a28ea94ccf988100",
            "bde4697110d042c9be70e148124b310d",
            "fccfdbe5fa92474ca4b357cebe308d55",
            "d9701d2dc2d542cab612e3d62fac9376",
            "de8e61550ce546cfb43ecf7eb90d5160",
            "270ccdffa8b24e4383f24fd82c1b578f",
            "692a99a3c86846b39ed367b7f5c73d6a",
            "5566a91327e841379dee3a5514b43188",
            "a69baa57c5d14e40a5ba03b60986fa25",
            "7d76b65fa77e47b294c4f4a8262f02d6",
            "b93b8cbe1207409b9f0d9e2dc5451452",
            "4b300b4658c04411a50fe3ca4e4b74d2",
            "09d9837f336543b1bdc61ef91b91edc1",
            "c671ca0a365a4f4a8fd66a3547393e6f",
            "c7e6adb0906e45589db64b5fd670e950",
            "385a8bd8a753425ab5a4ddb0714b3b7b",
            "9c77f49174e94ab7bcc1caa107663733",
            "f0bef87be9e848319ab437a415b5a079",
            "974b15dbdca34730a92a305b9dcb3015",
            "b9e53ee517d24a3a9420b7779ac32076",
            "3313c53e3ef746a681423fbedff7ee44",
            "26aa8b27d316434098c4735347fc0d35",
            "8995ac3743e649cd8d1059ca41a22a32",
            "6069a05b1ba347c59e7b1ae2a44306dc",
            "5af6f2aae5944cfead2630171b543558",
            "65e9ae3f914643e9a8a1c49183af3d0f",
            "e93a6853e5f14105834dcdffddd6e8ce",
            "83d836e4846e489daa887d87b6d817ce",
            "806ac29925d14b248080e106b7acd859",
            "096b4b2a5fe84f06b3ecfee1cc435cc8",
            "6c0614a14c4041fc99e95e3fb5650d38",
            "74c876ded3bb4f86945ad11278b79495",
            "16750bd876b5473ba0a4d583554f6fef",
            "2bd55d0d01d949de82aaefdfb24e0499",
            "ba3aa6595cdc4d3b84dd9adf4cf98d33",
            "d724f2efa4d74147a85752490f52a3bd",
            "9aa569b2ca694775b626fde684bc409c",
            "12409753facf4c5b8d851f469cc2949f",
            "1628cf7ddada4d8c9ae6d6130466738e",
            "b270497d341d4a6492746ba0f174f3fc",
            "c74d7f87a9874b51a90c5a3ffc23eb99",
            "0089842c9fbb4bf79471484e99c40d2a",
            "44c6f9abdf4b458ca102df9917978cb2",
            "4fc858dac84a45afa4cfb47308f3220b",
            "6fbf972f8a5c4f978c5b4cd14eef9a73",
            "f9efc7fdbffa4490b8376391ffce40c6",
            "683a34b9b7af4dfcb75f709036b53579",
            "816ecebafd944a1296d77562ae58d4b6"
          ]
        },
        "id": "EtP2eBdxDiGx",
        "outputId": "b981ba8c-8b94-4013-ad78-dadeca583619"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f47115ffc2f34374845ffc87d54b13b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da96a0fbb5dd45b6be79a693088d9ef2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74c05cad30614bd3b05d4da8a5838b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de8e61550ce546cfb43ecf7eb90d5160"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "385a8bd8a753425ab5a4ddb0714b3b7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e93a6853e5f14105834dcdffddd6e8ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12409753facf4c5b8d851f469cc2949f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Search**\n",
        "\n",
        "It suits occasion require fast inference but less quality.\n",
        "- Pros: fast\n",
        "- Cons: if a wrong word selected, it influences the subsequent prediction. The text generated is plain, easily fall into local minimum. (can use penalty)\n"
      ],
      "metadata": {
        "id": "CczzNZ_VMQQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "print(input_ids.shape)\n",
        "# this contains the token index for token vector (14 tokens including\n",
        "# punctuation) however further embedding will not use way like one-hot\n",
        "# vectors but embedding vector 768"
      ],
      "metadata": {
        "id": "Sy9sI_K1Et1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff6c664-3a03-4888-bfcd-349fe7cb1f63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(logits):\n",
        "    return torch.argmax(logits, dim=-1).unsqueeze(-1)"
      ],
      "metadata": {
        "id": "HUF6OK8J-NdE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lm_decoding(model, max_new_tokens, seq_ids, eos_token_id, decode_func):\n",
        "    # the step is to (1) take the model output, (2) use argmax to get the tocken id,\n",
        "    #  (3) concatenate the id to the end of the sequence\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(seq_ids)\n",
        "            next_token_logits = outputs.logits[:, -1, :] # the dimension is\n",
        "            # [batch size, token length, vocab size], so we take the last token,\n",
        "            # this logis is the value before softmax\n",
        "            # The argmax of the logits is equivalent to the argmax of the\n",
        "            # softmax probabilities.\n",
        "            next_token_id = decode_func(next_token_logits)\n",
        "\n",
        "            if next_token_id == eos_token_id:\n",
        "                print(\"The output sequence comes to an end.\")\n",
        "                break\n",
        "            seq_ids = torch.cat([seq_ids, next_token_id], dim=-1)\n",
        "\n",
        "    return seq_ids"
      ],
      "metadata": {
        "id": "FS7ndj8oEYRl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "decode_ids = lm_decoding(model, max_new_tokens, input_ids.clone(), tokenizer.eos_token_id, greedy_search)\n",
        "generated_text = tokenizer.decode(decode_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVBVdGxkGU0E",
        "outputId": "5aaf817b-0278-4a9b-fea4-5ce7ead9c8fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain,  the most difficult path to the top of the mountain.  The most difficult path to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam Search**\n",
        "\n",
        "It keeps num of beams (width) choices (when beam = 1, it is identical as greedy search). It is heristic method. The final output is the one with highest cumulative probability. This method tackled the limitation of greedy search.\n",
        "\n",
        "- Cons: the optimal sequence may not be selected and requires large space and computational resource.\n",
        "\n",
        "- Varitions:\n",
        "    * Speculative Beam Search for Simultaneous Translation https://doi.org/10.48550/arXiv.1909.05421\n",
        "    * Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models https://doi.org/10.48550/arXiv.1610.02424\n",
        "    * Cluster-based beam search for pointer-generator chatbot grounded by knowledge https://doi.org/10.1016/j.csl.2020.101094\n",
        "    * Beam-search SIEVE for low-memory speech recognition https://doi.org/10.21437/Interspeech.2024-2457\n",
        "\n",
        "\n",
        "\n",
        "`Reference:`\n",
        "\n",
        "- https://www.width.ai/post/what-is-beam-search\n",
        "- https://doi.org/10.48550/arXiv.1805.04833"
      ],
      "metadata": {
        "id": "gB5GDMNI0G46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(prompt, model, tokenizer, max_new_tokens=20, beam_size=3, alpha=0.7):\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    current_ids = input_ids.clone()\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    current_beam = [(current_ids, 0.0)] # init the current idx and probability\n",
        "    finished_sequence = []\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        beam_pool = [] #tmp list for beam\n",
        "\n",
        "        # except the initial iteration, the current beam has 3 sequence\n",
        "        for i_sequence, i_log_prob in current_beam:\n",
        "\n",
        "            if i_sequence[0,-1] == eos_token_id:\n",
        "                finished_sequence.append((i_sequence, i_log_prob))\n",
        "                continue\n",
        "            # for each beam it gets the top beam_size next token\n",
        "            with torch.no_grad():\n",
        "                outputs = model(i_sequence)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "                log_probs = torch.log_softmax(next_token_logits.ravel(), dim=-1)\n",
        "                top_k_log_probs, top_k_ids = torch.topk(log_probs, beam_size)\n",
        "\n",
        "            # loop over each new token\n",
        "            for i in range(beam_size):\n",
        "                token_id = top_k_ids[i]\n",
        "                log_prob = top_k_log_probs[i]\n",
        "\n",
        "                # cat the previous sequence plus one beam token\n",
        "                new_sequence = torch.cat([i_sequence, token_id.reshape(1,-1)], dim=1)\n",
        "                # cumulate the probability\n",
        "                beam_pool.append((new_sequence, log_prob + i_log_prob))\n",
        "\n",
        "        beam_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "        current_beam = beam_pool[:beam_size]# select the top 3 to the current beam\n",
        "\n",
        "    finished_sequence.extend(current_beam)\n",
        "\n",
        "    # Since probability is between 0 to 1 while log < 1 is alwasy\n",
        "    # negative numbers, so cumulative probability of a long sequence will have\n",
        "    # smaller prabability. The solution is to normalize\n",
        "    normalized_score_key = lambda x: x[1] / (x[0].size(1)**alpha) # by alpha\n",
        "    best_sequence = max(finished_sequence, key=normalized_score_key)\n",
        "\n",
        "    generated_text = tokenizer.decode(best_sequence[0].squeeze(), skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "f8wxYkbeKX4R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example 1\n",
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "print(f\"Example 1: greedy search with beam size of 1 \\n {beam_search(prompt, model, tokenizer, max_new_tokens=20, beam_size=1, alpha=0.7)}\\n\")\n",
        "\n",
        "# example 2\n",
        "prompt = \"The earth is the place\"\n",
        "print(f\"Example 2: beam search with size of 5\\n {beam_search(prompt, model, tokenizer, max_new_tokens=10, beam_size=5, alpha=0.5)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blTbDBY--Bd7",
        "outputId": "d624c1d6-b242-4452-93b1-87e380a1a1b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: greedy search with beam size of 1 \n",
            " There is more than one path to the top of the mountain,  the most difficult path to the top of the mountain.  The most difficult path to the\n",
            "\n",
            "Example 2: beam search with size of 5\n",
            " The earth is the place where the sun shines.”\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top K Sampling**"
      ],
      "metadata": {
        "id": "AtygrdOMeU2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "ej_uDAqMExgR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits, top_k):\n",
        "    # select top k candidates and set the threshold\n",
        "    top_k_log_probs, _ = torch.topk(logits, top_k)\n",
        "    min_k_prob = top_k_log_probs[:, -1]\n",
        "    # create a new logit by setting -inf to all probs below min_k_prob\n",
        "    new_logits = torch.where(next_token_logits < min_k_prob, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "    # renormalize the probs\n",
        "    normalized_probs = torch.softmax(new_logits, dim=-1)\n",
        "    return torch.multinomial(normalized_probs, num_samples=1)\n"
      ],
      "metadata": {
        "id": "HIDZwKfY_bvJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "eos_token_id = tokenizer.eos_token_id # end of sequence\n",
        "current_ids = input_ids.clone() # deep copy\n",
        "top_k = 10\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        next_token_id = top_k_sampling(next_token_logits, top_k)\n",
        "\n",
        "        if next_token_id == eos_token_id:\n",
        "            print(\"The output sequence comes to an end.\")\n",
        "            break\n",
        "\n",
        "        current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "QNIkJ0oweWto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c10c257-0cdf-4ae9-9842-13afa73bc46b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain,  a path to where one is not only able to find the right route to where one needs to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top P Sampling**\n",
        "\n",
        "Reference: https://arxiv.org/pdf/1904.09751"
      ],
      "metadata": {
        "id": "sfJ4M3KdFzrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "2H2ghuYDFyJT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(logits, top_p):\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    # sort and cumulate probs\n",
        "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    # thresholding\n",
        "    sorted_idx_remove = cum_probs > top_p\n",
        "    sorted_idx_remove[..., 1:] = sorted_idx_remove[..., :-1].clone()\n",
        "    sorted_idx_remove[..., 0] = False\n",
        "    # fill -inf to idx above threshold\n",
        "    index_remove = sorted_idx[sorted_idx_remove]\n",
        "    logits.scatter_(dim=-1, index=index_remove.unsqueeze(0), value=float(\"-inf\"))\n",
        "    # normalize and sample\n",
        "    normalized_probs = torch.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(normalized_probs, num_samples=1)"
      ],
      "metadata": {
        "id": "TBc7AjHx_9iY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "eos_token_id = tokenizer.eos_token_id # end of sequence\n",
        "current_ids = input_ids.clone() # deep copy\n",
        "top_p = 0.8\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        next_token_id = top_p_sampling(next_token_logits, top_p)\n",
        "\n",
        "        if next_token_id == eos_token_id:\n",
        "            print(\"The output sequence comes to an end.\")\n",
        "            break\n",
        "\n",
        "        current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "niKsP-9f92Vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47fcc9e9-71b9-4ae6-98fe-da66b5011422"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain, vernacular in the moment,‏‏‏‏‏‏‏\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Termperture**\n"
      ],
      "metadata": {
        "id": "qCi30C8UU6SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"There is more than one path to the top of the mountain, \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "1RsBnrUNN_IC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "eos_token_id = tokenizer.eos_token_id # end of sequence\n",
        "current_ids = input_ids.clone() # deep copy\n",
        "top_p = 0.8\n",
        "temperture_val = 0.7\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        # temperture + top k sampling\n",
        "        next_token_logits = next_token_logits / temperture_val\n",
        "        next_token_id = top_p_sampling(next_token_logits, top_p)\n",
        "\n",
        "        if next_token_id == eos_token_id:\n",
        "            print(\"The output sequence comes to an end.\")\n",
        "            break\n",
        "\n",
        "        current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktjmi3VQWExM",
        "outputId": "b6775698-b826-403d-e41a-8f13229d790a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is more than one path to the top of the mountain,   the one that leads to the very top of the mountain.  The one that leads to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eriCvI0QpFPv"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}
